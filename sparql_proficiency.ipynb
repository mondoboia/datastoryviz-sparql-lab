{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "603da6bb",
   "metadata": {},
   "source": [
    "# SPARQL proficiency\n",
    "\n",
    "The process of calculating the SPARQL proficiency metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb873863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "color_palette = ['#5565cf', '#b14ecf', '#cf4773', '#cf913f', '#87cf38', '#30cf6b', '#29a1cf', '#2138cf']\n",
    "\n",
    "file_path = 'data_stories_summary.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eb2cac",
   "metadata": {},
   "source": [
    "## Correct score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cae5059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define expected rules based on query type\n",
    "expected_variables_by_type = {\n",
    "    'table': None,  # No constraints on variable names, labels can be used\n",
    "    'count': ['count'],  # Must have ?count of type number\n",
    "    'barchart': ['label', 'count'],  # Option 1: ?label and ?count, Option 2: only ?label\n",
    "    'linechart': ['label', 'count'],\n",
    "    'doughnutchart': ['label', 'count'],\n",
    "    'scatterplot': ['x', 'y'],  # Must have ?x and ?y\n",
    "    'textsearch': None,  # No specific constraints, <<searchterm>> placeholder is normal\n",
    "    'map': ['point', 'lat', 'long']  # Must have ?point, ?lat, and ?long\n",
    "}\n",
    "\n",
    "# Function to test the SPARQL query and validate based on type\n",
    "def test_sparql_query(endpoint, query, default_search_term='\"example_search_term\"', max_retries=3, delay=30):\n",
    "    headers = {\n",
    "        'Accept': 'application/sparql-results+json'\n",
    "    }\n",
    "\n",
    "    # Replace the placeholder <<searchterm>> with the default search term\n",
    "    query = query.replace(\"<<searchterm>>\", default_search_term)\n",
    "\n",
    "    params = {\n",
    "        'query': query\n",
    "    }\n",
    "    \n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = requests.get(endpoint, headers=headers, params=params)\n",
    "            \n",
    "            # Check for any 2xx status code (successful responses)\n",
    "            if 200 <= response.status_code < 300:\n",
    "                print(f\"Query to {endpoint} succeeded with status code {response.status_code}.\")\n",
    "                \n",
    "                # Attempt to parse the JSON response\n",
    "                try:\n",
    "                    response_json = response.json()  # Ensure the response is valid JSON\n",
    "                    return True, response_json\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Failed to parse JSON for {query}. Response content: {response.text}\")\n",
    "                    return True, None  # Consider it a successful query but with invalid JSON\n",
    "                \n",
    "            elif response.status_code == 429:\n",
    "                print(f'Rate limit hit (429). Retrying in {delay} seconds...')\n",
    "                time.sleep(delay)\n",
    "                retries += 1\n",
    "            else:\n",
    "                print(f'{response.status_code} for {endpoint}, {query}')\n",
    "                return False, None\n",
    "        except Exception as e:\n",
    "            print(f'Exception for {endpoint}, {query}: {e}')\n",
    "            return False, None\n",
    "    return False, None\n",
    "\n",
    "# Function to check for query correctness based on result variables and type-specific rules\n",
    "def check_query_correctness(response_json, query_type):\n",
    "    # If the response_json is None, we cannot validate the correctness\n",
    "    if response_json is None:\n",
    "        print(f\"No valid JSON to check for query type {query_type}.\")\n",
    "        return False\n",
    "    \n",
    "    # Handling each type of query based on specific rules\n",
    "    if query_type == 'table':\n",
    "        print(\"Checking table query\")\n",
    "        return True\n",
    "    \n",
    "    elif query_type == 'count':\n",
    "        print(\"Checking count query\")\n",
    "        if 'head' in response_json and 'vars' in response_json['head']:\n",
    "            returned_vars = response_json['head']['vars']\n",
    "            if 'count' in returned_vars:\n",
    "                # Now check if the count is a number\n",
    "                if 'results' in response_json and 'bindings' in response_json['results']:\n",
    "                    for binding in response_json['results']['bindings']:\n",
    "                        if 'count' in binding and binding['count']['type'] == 'literal' and binding['count'].get('datatype') == 'http://www.w3.org/2001/XMLSchema#integer':\n",
    "                            return True\n",
    "                print(f\"Query did not return a valid number for ?count\")\n",
    "        return False\n",
    "\n",
    "    elif query_type in ['barchart', 'linechart', 'doughnutchart']:\n",
    "        print(f\"Checking {query_type} query\")\n",
    "        if 'head' in response_json and 'vars' in response_json['head']:\n",
    "            returned_vars = response_json['head']['vars']\n",
    "            if ('label' in returned_vars and 'count' in returned_vars) or ('label' in returned_vars):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    elif query_type == 'scatterplot':\n",
    "        print(\"Checking scatterplot query\")\n",
    "        if 'head' in response_json and 'vars' in response_json['head']:\n",
    "            returned_vars = response_json['head']['vars']\n",
    "            if 'x' in returned_vars and 'y' in returned_vars:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    elif query_type == 'textsearch':\n",
    "        print(\"Checking textsearch query\")\n",
    "        return True  # Assume correctness as the search term is dynamically replaced\n",
    "    \n",
    "    elif query_type == 'map':\n",
    "        print(\"Checking map query\")\n",
    "        if 'head' in response_json and 'vars' in response_json['head']:\n",
    "            returned_vars = response_json['head']['vars']\n",
    "            if 'point' in returned_vars and 'lat' in returned_vars and 'long' in returned_vars:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    else:\n",
    "        print(f\"Unrecognized query type: {query_type}\")\n",
    "        return False\n",
    "\n",
    "story_sparql_metrics = []\n",
    "\n",
    "# Iterate over the stories in the JSON\n",
    "for story_id, story in data.items():\n",
    "    sparql_endpoint = story['sparql_endpoint']\n",
    "    queries = story.get('queries', [])\n",
    "\n",
    "    total_queries = 0\n",
    "    working_queries = 0\n",
    "    correct_queries = 0\n",
    "\n",
    "    for query_dict in queries:\n",
    "        query = query_dict.get('query', '')\n",
    "        query_type = query_dict.get('type', '')\n",
    "        total_queries += 1\n",
    "        query = re.sub(r'(^|\\s)#.*?(\\r\\n|\\n)', r'\\1', query)\n",
    "        query = re.sub(r'\\s+', ' ', query).strip()\n",
    "\n",
    "        success, response_json = test_sparql_query(sparql_endpoint, query)\n",
    "        \n",
    "        if success:\n",
    "            working_queries += 1\n",
    "            # Check query correctness based on response variables and result content\n",
    "            if response_json:\n",
    "                if check_query_correctness(response_json, query_type):\n",
    "                    correct_queries += 1\n",
    "        # Delay of 5 seconds between queries to avoid hitting rate limits\n",
    "        time.sleep(5)\n",
    "\n",
    "    # Calculate the percentage of working queries\n",
    "    if total_queries > 0:\n",
    "        working_percentage = (working_queries / total_queries) * 100\n",
    "        correct_percentage = (correct_queries / total_queries) * 100\n",
    "    else:\n",
    "        working_percentage = 0\n",
    "        correct_percentage = 0\n",
    "    \n",
    "    # composite SPARQL proficiency score (normalized to [0, 1]\n",
    "    # proficiency_score = (working_percentage + correct_percentage) / 200.0\n",
    "    story_sparql_metrics.append({\n",
    "        'story_id': story_id,\n",
    "        'total_queries': total_queries,\n",
    "        'working_queries': working_queries,\n",
    "        'correct_queries': correct_queries,\n",
    "        'working_percentage': working_percentage,\n",
    "        'correct_percentage': correct_percentage,\n",
    "        # 'sparql_proficiency': round(proficiency_score, 2)\n",
    "    })\n",
    "\n",
    "    print(f\"Story ID: {story_id} | Total: {total_queries} | Working: {working_queries} \"\n",
    "          f\"({working_percentage:.2f}%) | Correct: {correct_queries} ({correct_percentage:.2f}%)\")\n",
    "\n",
    "df_sparql = pd.DataFrame(story_sparql_metrics)\n",
    "df_sparql.to_csv('outputs/sparql_queries_score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4d22c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sparql_queries = pd.read_csv('outputs/sparql_queries_score.csv', index_col=0)\n",
    "df_sparql_queries['combined_normalized'] = ((df_sparql_queries['working_percentage'] + df_sparql_queries['correct_percentage']) / 200.0).round(2)\n",
    "df_sparql_queries['correct_normalized'] = (df_sparql_queries['correct_percentage'] / 100.0)\n",
    "\n",
    "df_sparql_proficiency = df_sparql_queries[['story_id', 'combined_normalized', 'correct_normalized']].copy()\n",
    "df_sparql_proficiency.to_csv('outputs/sparql_proficiency_stage_one.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98005f6c",
   "metadata": {},
   "source": [
    "## Federated bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08f6e265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression to capture all federated endpoints in SERVICE clauses\n",
    "service_pattern = re.compile(r'SERVICE\\s*<([^>]+)>', re.IGNORECASE)\n",
    "\n",
    "# Initialize counters\n",
    "federation_metrics = []\n",
    "data_sources_list = []\n",
    "\n",
    "# Loop through the stories in the JSON data (per story)\n",
    "for story_id, story_data in data.items():\n",
    "    queries = story_data.get('queries', [])\n",
    "    \n",
    "    main_endpoint = story_data.get(\"sparql_endpoint\", \"\")\n",
    "    if main_endpoint:\n",
    "        data_sources_list.append(main_endpoint.split(\"//\")[1])\n",
    "    \n",
    "    total_queries_story = 0\n",
    "    federated_queries_story = 0\n",
    "    \n",
    "    # Loop through each query in the story\n",
    "    for query_dict in queries:\n",
    "        total_queries_story += 1\n",
    "        query = query_dict.get('query', '')\n",
    "        query = re.sub(r'(^|\\s)#.*?(\\r\\n|\\n)', r'\\1', query)\n",
    "        query = re.sub(r'\\s+', ' ', query).strip()\n",
    "        \n",
    "        service_matches = service_pattern.findall(query)\n",
    "        if service_matches:\n",
    "            federated_queries_story += 1\n",
    "    \n",
    "    # Calculate federated bonus for this story (fraction of federated queries)\n",
    "    federated_bonus = (federated_queries_story / total_queries_story) if total_queries_story > 0 else 0\n",
    "    \n",
    "    # Store per-story federation metrics (using story_id as key)\n",
    "    federation_metrics.append({\n",
    "        'story_id': story_id,\n",
    "        'total_queries_story': total_queries_story,\n",
    "        'federated_queries_story': federated_queries_story,\n",
    "        'federated_bonus': federated_bonus\n",
    "    })\n",
    "\n",
    "\n",
    "df_federation = pd.DataFrame(federation_metrics)\n",
    "df_federation.to_csv('outputs/sparql_federation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dad49faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proficiency = pd.read_csv(\"outputs/sparql_proficiency_stage_one.csv\", index_col=0)\n",
    "df_federation = pd.read_csv(\"outputs/sparql_federation.csv\", index_col=0)\n",
    "\n",
    "df_merged = df_proficiency.merge(df_federation['federated_bonus'], left_index=True, right_index=True)\n",
    "\n",
    "df_merged.to_csv(\"outputs/sparql_proficiency_stage_two.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6209e4d",
   "metadata": {},
   "source": [
    "## SPARQL Features Complexity and Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b182c6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights based on usage frequency\n",
    "\n",
    "sparql_features = [\"FILTER\", \"OPTIONAL\", \"BIND\", \"UNION\", \"MINUS\", \"COUNT\", \n",
    "                   \"DISTINCT\", \"LIMIT\", \"OFFSET\", \"ORDER BY\", \"GROUP BY\", \"SUM\", \"SAMPLE\", \n",
    "                   \"GROUP_CONCAT\", \"VALUES\", \"STR\", \"LANG\", \"IF\", \"CONTAINS\", \"REPLACE\", \"ROUND\", \"ASC\", \"DESC\"]\n",
    "\n",
    "\n",
    "def count_sparql_features(query, features):\n",
    "    \"\"\"\n",
    "    Count the occurrences of each feature in the query using regex word boundaries.\n",
    "    Returns a dictionary with the counts.\n",
    "    \"\"\"\n",
    "    feature_counts = {feature: len(re.findall(r'\\b' + feature + r'\\b', query, re.IGNORECASE)) \n",
    "                      for feature in features}\n",
    "    return feature_counts\n",
    "\n",
    "def clean_query_text(query):\n",
    "    # Remove comments\n",
    "    query = re.sub(r'(^|\\s)#.*?(\\r\\n|\\n)', r'\\1', query)\n",
    "    # Normalize whitespace\n",
    "    query = re.sub(r'\\s+', ' ', query)\n",
    "    # Remove variables like ?varName\n",
    "    query = re.sub(r'\\?[^\\s\\)\\(]*', '', query)\n",
    "    # Remove URIs like <http://example.org/prop>\n",
    "    query = re.sub(r'<[^>]*>', '', query)\n",
    "    return query.strip()\n",
    "\n",
    "\n",
    "# global features usage\n",
    "global_feature_counts = {feature: 0 for feature in sparql_features}\n",
    "total_feature_instances = 0\n",
    "\n",
    "for story_data in data.values():\n",
    "    queries = story_data.get('queries', [])\n",
    "    for query_dict in queries:\n",
    "        query = query_dict.get('query', '')\n",
    "        query = clean_query_text(query)\n",
    "        # Count features for this query.\n",
    "        feature_counts = count_sparql_features(query, sparql_features)\n",
    "        for feature, count in feature_counts.items():\n",
    "            global_feature_counts[feature] += count\n",
    "            total_feature_instances += count\n",
    "\n",
    "feature_percentages = {}\n",
    "# percentage for each feature\n",
    "if total_feature_instances > 0:\n",
    "    for feature in sparql_features:\n",
    "        count = global_feature_counts[feature]\n",
    "        percentage = count / total_feature_instances\n",
    "        feature_percentages[feature] = percentage\n",
    "else:\n",
    "    for feature in sparql_features:\n",
    "        feature_percentages[feature] = 0.0\n",
    "\n",
    "# inverted weights\n",
    "feature_weights = {}\n",
    "for feature in sparql_features:\n",
    "    weight = 1 - feature_percentages[feature]\n",
    "    feature_weights[feature] = weight\n",
    "\n",
    "# normalise\n",
    "min_w = min(feature_weights.values())\n",
    "max_w = max(feature_weights.values())\n",
    "if max_w > min_w:\n",
    "    for feature in sparql_features:\n",
    "        weight = feature_weights[feature]\n",
    "        normalised = (weight - min_w) / (max_w - min_w)\n",
    "        feature_weights[feature] = normalised\n",
    "\n",
    "story_feature_metrics = {}\n",
    "for story_id, story_data in data.items():\n",
    "    queries = story_data.get('queries', [])\n",
    "    total_queries = len(queries)\n",
    "    total_feature_counts = {feature: 0 for feature in sparql_features}\n",
    "    diversity_sum = 0.0\n",
    "    \n",
    "    for query_dict in queries:\n",
    "        query = query_dict.get('query', '')\n",
    "        query = clean_query_text(query)\n",
    "        # Count features for this query.\n",
    "        feature_counts = count_sparql_features(query, sparql_features)\n",
    "        for feature, count in feature_counts.items():\n",
    "            total_feature_counts[feature] += count\n",
    "\n",
    "        # Calculate diversity for this query\n",
    "        total_features_in_query = sum(feature_counts.values())\n",
    "        unique_features_in_query = sum(1 for count in feature_counts.values() if count > 0)\n",
    "        diversity_score = unique_features_in_query / len(sparql_features)\n",
    "\n",
    "        diversity_sum += diversity_score\n",
    "        \n",
    "    weighted_sum = sum(total_feature_counts[feature] * feature_weights.get(feature, 1) for feature in sparql_features)\n",
    "    # Compute the simple average metric: total features divided by number of queries.\n",
    "    feature_metric = weighted_sum / total_queries if total_queries > 0 else 0.0\n",
    "    \n",
    "    # Calculate average diversity metric\n",
    "    diversity_metric = diversity_sum / total_queries if total_queries > 0 else 0.0\n",
    "    \n",
    "    # Store the metrics for the story using its story_id.\n",
    "    story_feature_metrics[story_id] = {\n",
    "        'total_queries': total_queries,\n",
    "        'weighted_feature_sum': weighted_sum,\n",
    "        'sparql_feature_metric': feature_metric,\n",
    "        'diversity_metric': diversity_metric\n",
    "    }\n",
    "\n",
    "# Convert the metrics dictionary into a DataFrame.\n",
    "df_features = pd.DataFrame.from_dict(story_feature_metrics, orient='index')\n",
    "df_features.index.name = 'story_id'\n",
    "df_features.reset_index(inplace=True)\n",
    "\n",
    "# Normalize the sparql_feature_metric using min-max normalization\n",
    "min_metric = df_features['sparql_feature_metric'].min()\n",
    "max_metric = df_features['sparql_feature_metric'].max()\n",
    "\n",
    "if max_metric > min_metric:\n",
    "    df_features['normalized_sparql_feature_metric'] = (\n",
    "        (df_features['sparql_feature_metric'] - min_metric) / (max_metric - min_metric)\n",
    "    )\n",
    "else:\n",
    "    df_features['normalized_sparql_feature_metric'] = 0\n",
    "\n",
    "df_features.to_csv('outputs/sparql_features_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14dd542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proficiency = pd.read_csv(\"outputs/sparql_proficiency_stage_two.csv\", index_col=0)\n",
    "df_features = pd.read_csv(\"outputs/sparql_features_scores.csv\", index_col=0)\n",
    "\n",
    "df_merged = df_proficiency.merge(df_features[['normalized_sparql_feature_metric', 'diversity_metric']], left_index=True, right_index=True)\n",
    "df_merged.to_csv(\"outputs/sparql_proficiency_stage_three.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d56f0a",
   "metadata": {},
   "source": [
    "## Final Proficiency Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c775664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sparql_proficiency(row):\n",
    "\n",
    "    weight_wc = 0.1      # correct queries, we already know that most of them works\n",
    "    weight_fed = 0.1     # shows advanced usage with federated queries\n",
    "    weight_feat = 0.4    # features usage reflects query complexity and versatility\n",
    "    weigh_diversity = 0.4\n",
    "    \n",
    "    proficiency = (row['correct_normalized'] * weight_wc +\n",
    "                   row['federated_bonus'] * weight_fed +\n",
    "                   row['normalized_sparql_feature_metric'] * weight_feat +\n",
    "                   row['diversity_metric'] * weigh_diversity)\n",
    "    \n",
    "    return proficiency\n",
    "\n",
    "df_proficiency_final = pd.read_csv(\"outputs/sparql_proficiency_stage_three.csv\", index_col=0)\n",
    "df_proficiency_final['sparql_proficiency'] = df_proficiency_final.apply(compute_sparql_proficiency, axis=1)\n",
    "\n",
    "df_proficiency_final.to_csv(\"outputs/sparql_proficiency_final.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
